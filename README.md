# high-performance-programming
A collection of hands-on experiments exploring CPU and GPU performance optimization techniques. Topics include memory hierarchy, pipelines, vectorization, multithreading, OpenMP, and GPU parallelism.
## Lab Overviews
### Lab 0: Performance Evaluation
- Documenting machine characteristics: type of CPU, operating frequency of cores, levels of cache and their characteristics, microarchitecture of processor cores, memory bandwidth, Base Frequency and Turbo Frequency.
- Computer's self measurement of time: determining accuracy of timers, researching RDTSC-based methods, calibrating timers, documenting resolution, accuracy, and precision.
- Performance evaluation using Cycles-per-Element (CPE): creating plots of measurements, adding depth and breadth to deal with noisy calculations, estimating CPE by slope.
- Interacting with the compiler: Observing difference in assembly language code for different level of compiler optimizations.
- Generating roofline plots: analyzing code for strategic optimization effort, using different Arithmetic Intensites (AI)
  
### Lab 1: Memory Optimization
- Memory bandwidth profiling with script that compiled C program, measuring memory-copy speed.
- Testing code transformations of matrix combine, multiply, and transpose, capturing data for number of cycles, speedup, and ratio of number of cycles to number of elements.
- Optimizing and analyzing Matrix-Matrix Multiplication (MMM) using loop interchange.
- Optimizing and analyzing MMM using blocking.
- Optimizing and analyzing Matrix Transpose using loop interchange and blocking.

### Lab 2: Optimization Methods
- Using methods of optimizing basic blocks accounting for basic characteristics of the compiler and microarchitecture, including dealing with optimization blockers such as FP pipeline latency and conditional moves.
- Exploring methods of code motion, loop unrolling, accumulation, and branch promotion. Extending understanding of branch prediction and memory latency.
- Experimenting with basic optimization methods of Matrix Combine, with compiling optimizers, loop unrolling, multiple accumulators, and reassociative transformations.
- Applying basic methods to dot product with loop unrolling and multiple accumulators.
- Force and evaluate conditional moves: forcing data dependent behavior to enhance CPU optimizations that have data-dependent performance.

### Lab 3: Vectorization Intrinsics
- Small-scale vector programming using AVX.
- SSE extensions using C structs and union, vector unrolling with accumulators and different data type -- analyzed by CPE.
- SSE extensions using intrinsics: observing difference in performance with unaligned local allocation, unaligned heap, and aligned heap performance for computing square roots.
- Creating non-vectorized functions as baseline and vectorizing to make throughput optimial.
- Creating vectorized dot product using intrinsics.
- SSE application: matrix transpose using intrinsic \_MM_TRANSPOSE4_PS, combined with blocking and compiler optimizations, as well as performing a 4x4 transpose of double values using AVX_mm_256_\* operations.

### Lab 4: PThreads Tutorial
- Practicing multithreaded programming, testing the basics of PThreads: create, passing parameters, join, sync, and barriers.
- Benchmarking 1D Successive Over-Relaxation (SOR) with barriers.

### Lab 5: PThreads SOR
- Benchmarking different serial implementations of 2D SOR, finding optimal OMEGA as a function of array size.
- Serial SOR optimizations: red/black method, indies reversed, and blocked, all benchmarked to find the affect in performance, using time per innermost loop iteration to analyze results.
- Cost of multithreading: finding the overhead of PThreads' minimum necessary operations for threading (creating threads, passing parameters, and join).
- Multithreaded SOR: creating and evaluating, with respect to serial baseline, multithreaded versions of SOR with two array sizes -- one where the array fits in L3 cache and one where it does not. Optimizations with decomposition by strips.

### Lab 6: OpenMP
- Practicing using OpenMP, understanding the limits of Shared Address Space parallelizations.
- OpenMP basics: finding the intrinsic OpenMP overhead, compared to PThreads overhead, moving loop indices for MMM out of private list and into shared list, as well as moving the pragma around different nested loops to observe change in performance.
- Optimizing and parallelizing SOR and MMM with OpenMP.

### Lab 7: GPU SOR
- Set up and testing a GPU environment.
- Single Block: First method has each thread handles one output element, creating and initializing array, transferring to GPU, creating a single block, and each thread operates on a single output element. Second method has each thread handling multiple output elements, creating and initializing array on host, transferring to CPU, creating single block. Confirms correctness and finds maximum difference between elements generated by the CPU and GPU.
- Multiple Blocks: trying two different block sizes, 16x16 and 32x32. Varying number of blocks, changing the kernel call and array size accordingly, modifying previous code so that the interations are controlled within the GPU rather than the host to analyze difference in performance.

### Lab 8: GPU MMM Memory
- Using standard GPU features such as shared memory to write a useful GPU program, continuing exploration of MMM.
- Global Memory, Shared Memory, and Shared Memory with loop unrolling: Implement, test, and verify MMM, validated on the host, analyzing execution time as end-to-end including data transfers, and just the MMM execution time.
- Changing code using knowledge of coalescing memory accesses (global) and how memory banks work (shared), changing performance with: global memory accesses not coalesced, but shared memory accesses still good, global memoory accesses coalesced, but shared memory accesses have conflicts, and global memory accesses not coalesced and shared memory have conflicts.
